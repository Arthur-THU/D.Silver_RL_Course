
# 第二课 马尔科夫决策过程

课程ppt已上传为pdf。

## 马尔科夫决策过程介绍

* 马尔科夫决策过程形式上定义了强化学习的环境
* 这个环境是完全可观察的
* 当前的状态可以完全的表示过程
* 几乎所有的强化学习问题都可以转化为马尔科夫决策过程

## 马尔科夫性质

* 前一课笔记已经说过了，马尔科夫性质就是未来的状态只依赖于当前，与过去无关。

![](../images/7.png)

* 状态捕捉了历史中所有相关的信息，所以知道了状态，历史(history)就可以完全丢弃了
* 状态是未来的充分统计

## 状态转移矩阵

状态转移矩阵是马尔科夫过程中状态之间转移的概率所组成的矩阵，因此大小是状态数n的平方。

![](../images/8.png)

显然，每一行和为1

## 马尔科夫过程

马尔科夫过程是一个无记忆的随机过程。就像是一序列随机的满足马尔科夫性质的状态S1，S2，S3，...

![](../images/9.png)

举个例子，学生马尔科夫过程，马尔科夫过程也叫马尔科夫链。如下图：

![](../images/10.png)

图中包含了马尔科夫链的两个元素<S，P>，状态和状态转移矩阵。本例中，状态转移矩阵如下：

![](../images/11.png)

## 马尔科夫奖励过程

原本的马尔科夫过程加上奖励值之后就变成了马尔科夫奖励过程，元素由<S,P>变为<S,P,R,gamma>，
其中R就是奖励，gamma是折扣因子。

![](../images/12.png)

所以之前的学生马尔科夫过程相应的变成马尔科夫奖励过程，

![](../images/13.png)

红色标注的值就是奖励，注意的是，这里的奖励是即时奖励，也就是脱离状态时立刻得到的奖励，
无论你下个状态是什么。

## 回报

回报是从相应时间t开始以后的总的折扣奖励之和。

![](../images/14.png)

折扣因子在0和1之间，越靠近0，代表越近视，越注重眼前的利益，反之越接近1，代表眼光越长远，
这里并不代表不再看重眼前利益了，而是对未来的奖励和眼前的利益看重程度更加接近了。极限情况就是1，
代表对未来和现在一视同仁。

加入折扣因子的原因大致如下：
* 数学上计算的方便
* 避免未来进入循环马尔科夫过程带来的无限大回报
* 未来的不可靠性
* 类比经济学中，眼前的利益比未来的利益更加有意义
* 人类的行为也更加倾向于眼前利益
* 退一步来说，令gamma为1，也可以简单的转化成没有折扣因子的状态

## 价值函数

价值函数是状态S的长期价值表示。它是上面说的回报的期望。

![](../images/15.png)

它也会随着折扣因子的变化而取不同的值，因为回报会因为gamma的值而改变，期望自然也就会改变。

![](../images/16.png)

上图是gamma为1时，学生马尔科夫奖励过程，红色的就是状态的价值。

## 马尔科夫奖励过程的贝尔曼方程

贝尔曼方程用来方便的表示和计算马尔科夫奖励过程，价值函数可以分为两个部分;
* 即时奖励
* 下一状态的折扣状态价值

![](../images/17.png)

上面是贝尔曼方程的简单推导，比较简单，就不多说了。

我们可以用一种树的方式更好的理解贝尔曼方程的计算。

![](../images/18.png)

这样计算的话，我们试试验证刚才上面的学生马尔科夫奖励过程中的价值是否正确。

![](../images/19.png)

红色的部分计算发现没有问题，可以自行计算其他位置，都没有问题。

贝尔曼方程还可以表示成矩阵的形式，并且可以直接求逆进行计算：

![](../images/20.png)

但是可惜的是，有几个问题;
* 计算复杂，O(n3)复杂度
* 只能用于小型的MRP，即马尔科夫奖励过程

所以对于大型的MRP，一般用迭代的方式求解贝尔曼方程：
* 动态规划
* 蒙特卡洛方法
* 差分学习法

## 马尔科夫决策过程MDP

到目前为止其实我们都没有讲到强化学习，因为我们虽然对原始的MP引入了奖励R，变成了MRP，可是我们并没有
决策的部分，强化学习本身是一个决策的问题。所以现在我们再引入一个因子，就是Action，动作，MRP变成了
MDP，此时才能算得上是强化学习。MDP是一个环境，里面的每个状态都满足马尔科夫性质。

![](../images/21.png)

老规矩，上面的学生马尔科夫奖励过程中也加入A，变成学生MDP。

![](../images/22.png)

红色的部分就是Action。

## 策略policy

策略是状态到动作的映射，在某个状态下采取什么样的动作，可以是确定的策略，也可以是一个概率事件。

* MDP的策略只依赖于当前的状态，不依赖于history
* 策略是静态的，不受时间约束

## 价值函数

MDP的价值函数和MRP的有一些不同，与策略相关。而且分为状态价值函数和动作价值函数，定义如下：

![](../images/23.png)

学生MDP的状态价值如下图：

![](../images/24.png)

## 贝尔曼期望方程

和MRP类似，贝尔曼期望方程用来表示MDP。

![](../images/25.png)

同样类似的，可以用类似树结构来理解：

![](../images/26.png)

![](../images/27.png)

不同得是，从之前的看一步，到现在的看两步。黑色的点就是Q，白圈就是V。

也可以表示成矩阵的形式并直接求逆求解，问题同前面的一样。

## 最优价值函数

定义如下，最优价值函数指定了在MDP中可能的最好表现：

![](../images/28.png)

所以我们说知道了最优函数，不管是状态价值函数还是动作价值函数，MDP问题就解决了。
或者说这个强化学习问题就解决了。因为我们只需要挑最优的路径就行了。

## 最优策略

最优策略存在定理：

![](../images/29.png)


## 贝尔曼最优方程

贝尔曼最优方程：

![](../images/30.png)

![](../images/31.png)

* 贝尔曼最优方程非线性的
* 没有直接的解法
* 迭代的解决办法
    
    * 价值迭代
    
    * 策略迭代
    
    * Q-learning
    
    * Sarsa




