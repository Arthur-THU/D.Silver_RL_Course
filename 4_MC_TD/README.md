
# 第四课 免模型预测

课程ppt已上传为pdf

## 免模型强化学习

这节课开始讲免模型强化学习，所谓免模型就是MDP是未知的，包括状态转移矩阵P和奖励R。至于上节课的动态规划方法，
也就是策略迭代和价值迭代都是基于模型的方法，其实也就是一种理想化的方法，它是建立在我们完全知晓MDP的情况下的。
但是我们都知道，在现实里，这种情况是不多的。

首先这节课主要是免模型预测，下节课讲解的是免模型控制。

## 蒙特卡洛方法(MC)

首先是免模型方法中第一种方法，也就是蒙特卡洛方法，了解算法的人都应该知道蒙特卡洛方法，它其实就是一种随机方法，
精髓在一个“蒙”字，本质其实是以频率逼近概率。

强化学习中的蒙特卡洛方法有如下特点：

* 直接总经验片段学习
* 是免模型的，不需要MDP 转移矩阵和奖励的知识
* 从完整的片段中学习，没有自助抽样学习(bootstrapping)
* 使用最简单的可能思想：价值等于平均回报
* 提醒：MC只能在片段化的MDP中使用，而且每个片段必须是终止的，也就是要达到结束状态

## 蒙特卡洛策略评估

* 目标是从π下的经验片段中学习Vπ
* 蒙特卡洛方法用经验平均回报代替期望回报

大致分为两种：初访蒙特卡洛策略评估和每次访问蒙特卡洛策略评估

### 初访蒙特卡洛策略评估

* 目标：评估状态S的价值
* 在一个片段中第一次时间步t访问这个状态
* 增加计数N(s)<-N(s)+1
* 增加总的回报S(s)<-S(s) + Gt
* 价值被估计为平均回报V (s) = S(s)/N(s)
* 由大数定理可知，当计数趋于无穷大的时候，V (s) = vπ(s)

### 每次访问蒙特卡洛策略评估

和初访的区别就是，在一个经验片段中，每次访问这个状态都计数加一，而不是只在第一次增加。
其他计算过程没有差别。

### 增加平均

由上面的定义可知，我们需要不断地计算平均值以获得平均回报，所以我们不仅要保存计数还要保存回报总和，
很麻烦，因此可以对公式进行变换：

![](../images/37.png)

变换很简单，不多说了。因此上面的蒙特卡洛策略评估也修改如下：

![](../images/38.png)

可以看到我们现在只需要保存计数就可以了，价值函数可以一直保持最新。这样计算也更加方便。

* 在一些不稳定的问题中，计数平均 1/N 经常用滑动平均alpha来代替，可以忘记较老的片段。

## 时序差分学习(TD)

TD是另一种免模型的方法，一共会讲解三个方法，最后一个其实是介于TD和MC的过渡方法。

* TD和MC一样，也是直接从经验片段中学习
* 是免模型的
* 从不完整的片段中学习，利用bootstrapping，和MC的区别在这
* 用一个估计来更新价值函数的估计

MC每一次从片段中学习都是一直走到结束，得到总的折扣回报，而TD则是只向前走一步，然后估计后面的回报，
主要利用贝尔曼方程来估计。给出MC和TD的更新公式对比就很清晰了：
* MC：V (St) = V (St) + α (Gt − V (St))
* TD：V (St) = V (St) + α (Rt+1 + γV (St+1) − V (St))

Gt和Rt+1 + γV (St+1)我们通常称为目标项，alpha后面括号里的整体我们称为误差项
因此MC和TD的主要区别就是目标项的不同。为了比较，我把动态规划DP在放一起，
动态规划也是只向前看一步，和TD相似，那么区别是什么呢？我们知道DP是基于模型的，而
TD和MC都是免模型的，所以DP是一种全宽度搜索的方法，而MC和TD都是抽样方法，他们只是从MDP中
抽样学习，也就是从经验片段中学习。给出三幅图，我相信你就很清楚了。

|    MC | TD   | DP   |
|:-------:|:-----:|:-----:|
|![](../images/39.png)|![](../images/40.png)|![](../images/41.png)|

所以有一个直观的感受就是：MC必须等倒结束才能更新价值，而TD只需要向前一步就可以更新了。
如果你在开车的吗，这个片段的结局是撞车，那么MC就是你必须等撞车了，才能更新你现在这个状态的价值，
而TD可以立马改变自己的状态价值。

## MC 和 TD 的优劣

* TD可以在最终结果出来前学习
    
    * TD可以在每一步在线学习  
    
    * MC必须等倒片段的最后
    
* TD 不需要等到最终结果出来  
    
    * TD可以从不完整的序列中学习
    
    * MC必须从完整的序列中学习
    
    * TD在持续的环境中工作（没有终止）
    
    * MC只在片段式环境中工作（有终止）

* MC 高方差，0偏差
    
    * 好的收敛性质
    
    * 对初始价值不是很敏感
    
    * 理解简单，使用简单
    
* TD低方差，一些偏差
    
    * 通常比MC更高效
    
    * TD（0）更新至vπ(s)
    
    * 对初始化的价值更敏感

## 例子

下面讲解一个简单的例子，来理解MC和TD的区别：

两个状态A,B，没有折扣，8个经验片段
```
A, 0, B, 0
B, 1
B, 1
B, 1
B, 1
B, 1
B, 1
B, 0
```
那么V (A)和V (B)?

我们现在只算V（A）来体会下区别：

* MC来计算的话，V (St) = V (St) + 1/N * (Gt − V (St))，只有第一个片段出现，访问了一次，因为后面的奖励都是0，所以Gt为0,假设V (A)都初始化0，那么
V (A) = 0 + 1/1 * (0 - 0)=0

* TD，V (St) = V (St) + 1/N * (Rt+1 + γV (St+1) − V (St)),但是TD不拘泥于1个单独的片段。
把上述问题的MDP画出，如下：

![](../images/42.png)

所以，V = 0 + 1/1 * (0 + (0.75\*(1+0)+0.25\*(0+0))-0) = 0.75

总结一下就是：

* MC 收敛于最小化观察回报的最小二乘误差，它不开发马尔科夫性质，因此在非马尔科夫环境下更有效
* TD 收敛于最大似然马尔科夫模型，它开发马尔科夫性质，所以通常在马尔科夫环境下更有效

需要根据ppt好好理解一下。


## n步预测

最后一个方法就是介于TD和MC之间的过渡方法了。因为MC一直走到尾，TD只看一步，那么我们就很自然的想，能不能让TD多走几步。

![](../images/43.png)

相应的计算：

![](../images/44.png)

但是这样的效果并不是很好。所以更进一步，就出现了不止走一次，我多走几次，每次走不同的步，然后求平均或者加权平均。
λ-return就出现了。

![](../images/45.png)

这是一种Forward-view 方法，但是弊端也很明显，就是像MC一样，也必须要完整的片段了。

还有一种Backward view方法，它和Forward-view 方法是等价的，从不完整的序列在线更新。

课件中两句话表明了两者的关系：
* Forward view provides theory
* Backward view provides mechanism

## TD(λ)

最终形成了这个TD(λ)算法，就是我们说的TD与MC的过渡方法，TD（0）是其中的特殊情况。
这个算法需要好好根据ppt理解，我先停在这里，之后会给出更加详细的说明。因为后面的Sarsa(λ)也是同样的思想。







