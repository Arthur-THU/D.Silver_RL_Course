
# 第七课 策略梯度算法

课程ppt已上传为pdf。

## Policy-Based 强化学习

前面我们的课程其实都是Value-based learning，就是我们都是先进行策略的评估，也就是计算出值函数的值，V或者Q值，
然后再根据具体的V或者Q值按照greedy或者epsilon-greedy进行策略的选择，就像前面说的广义策略迭代一样。

但是其实这些方法都是有些绕了，为什么呢？因为我们的目的就是知道策略，最佳策略。在强化学习中，行动才是最终的目的。
但是前面的方法都是先求价值，然后再含蓄的从价值推出动作。那么我们可不可以不看价值，直接对策略进行求解分析呢？
这就是基于策略的强化学习，即Policy-Based RL。

## Policy-Based RL的优缺点

优点：
* 更好的收敛性质，因为是直接对策略进行操作的，而基于值的方法稍微绕了一些。
* 在高维和连续的动作空间里很有效。因为基于值的方法的话，必须要从价值中进行最大值的运算，动作很多的情况下很低效。
* 可以学习到随机的策略，这个很重要，下面会说到。

缺点：
* 比起全局最优，更倾向于收敛到局部最优
* 对策略进行估计很低效，而且高方差

## 随机策略的重要性

上面说到基于策略的RL可以学习到随机策略，那么为什么我们要学习随机策略呢？确定的策略哪边不好？
下面就以两个例子来说明。

* 剪刀石头布

剪刀石头布是完全随机的，因此用一个确定的策略，比如只出石头，是不太合理的，随机的策略更加合理。

* 方格世界





