
# 第九课 探索与开发


课程ppt已上传为pdf。

## 简介

本章主要讲解了强化学习中一个基本的权衡问题，就是探索与开发的问题，这是任何一个强化学习算法或者说项目都要考虑的问题，
否则你的算法效果肯定不会很好的。

### 探索与开发

这两者本身是一种冲突，开发要求我们对算法目前学习到的东西更加充分的加以利用，而探索则需要我们放弃之前学到的东西，
去尝试新的东西以求能否获得更好的效果。下面举几个例子：

* 经典的饭馆问题，你选择饭馆是选择去你以前经常吃的口味还不错的店呢？还是偶尔灵光一闪，去尝试一下新的店，这当然有风险，要么没之前的店好吃，但是也有可能
出乎意料的找到了一家更好的店。
* 在线广告展示问题，同样也是探索与开发的权衡，你是展示目前来说最成功的的广告，还是尝试新的不同的广告，效果当然就不能确定了。
* 游戏问题，选择你相信的最优动作，还是尝试新的动作，道理都是一样的。

## 几种探索与开发的方式

* 朴素探索：就像我们之前一直使用的e-greedy算法，很简单，但是很多情况下很有效。
* 乐观初始估计：优先选择当前被认为是最高价值的行为，除非新信息的获取推翻了该行为具有最高价值这一认知。
* 乐观面对不确定性：倾向于选择不确定性高的。
* 概率匹配：根据当前估计的概率分布采样行为。
* 信息状态搜索： 将已探索的信息作为状态的一部分联合个体的状态组成新的状态，以新状态为基础进行前向探索。

## 多臂赌博机

下面通过一个具体的例子，也是十分经典的例子，多臂赌博机来引入一些研究的概念。

![](https://github.com/cryer/D.Silver_RL_Course/raw/master/images/200.png)

如上图，这是多臂赌博机的基本情况：
* 每个赌博机相互独立，每个都有一个奖励的独立分布
* 拉下手臂即选择一个动作a，赌博机返回一个奖励r,一个episode就结束，没有状态
* 目标就是随着时间的进行，最大化累计得到的奖励

### 后悔值

* 动作价值Q（a）是一个动作a，也就是拉了哪个赌博机的手臂，获得的奖励的期望Q(a) = E [r|a]
* 最优的价值V*=Q(a∗) = max（Q（a））
* 后悔值就是最优值和当前动作价值的差值：lt = E [V ∗ − Q(at)]
* 总的后悔值就是上式在时间上的累计。

所以我们现在的目标不再是最大化累计奖励了，而是最小化总的后悔值，表面上看好像区别不大，其实不然，
后悔值的引入是很关键的一步，最明显的改变就是，最大化累计奖励的话，算法的提升不是很明显，而最小化后悔值的话，
累计后悔值的变化很明显，很容易比较分析，而且后面后悔值其实有一个最小下限的，是一个对数函数，这样就更加方便分析算法了。
我们目标就是找一个算法是的后悔值的累计越贴近这个对数函数下限越好。

### 计数后悔值

把后悔值的形式重新表达一下，引入计数N（a），代表动作a被选择的次数，delta代表最优值和当前动作Q值的差值，一个Gap。
然后后悔值变成如下的格式：

![](https://github.com/cryer/D.Silver_RL_Course/raw/master/images/201.png)

我们可以看出，一个好的算法就是要保证大的gap的手臂选择次数要少。但是有一个问题，就是gap其实我们不知道，因为最优值V*
我们是不知道的，而且因为奖励的分布我们也不一定知道，所以Q（a）也是未知的，但是我们可以利用经验估算，可以直接利用蒙特卡洛算法
估算Q（a），这个前面章节说的很清楚了，就是取平均本质。

### 线性和对数后悔值

下面这张图是三个基本算法的后悔值变化情况。

![](https://github.com/cryer/D.Silver_RL_Course/raw/master/images/202.png)

greddy和e-greedy都是线性的后悔值，decaying e-greedy是对数形式的后悔值，而且这其实就是前面说的后悔值的下限。

## 乐观面对不确定性

关于乐观初始估计和greddy方法我就不多说了，可以看ppt了解一下，我直接说这个乐观面对不确定性的方法，顾名思义，
就是要让我倾向于选择不确定性高的，因为不确定性高，所以即使最小的奖励很小，但是也有可能获得更大的奖励。
下面通过一个例子来讲解


![](https://github.com/cryer/D.Silver_RL_Course/raw/master/images/203.png)

上图是三个赌博机的奖励分布，当然我要强调的是这不是真实的奖励分布，而是你采样来的，也就是经验数据得到的。那么你会选择那一个赌博机呢？
答案是蓝色的，而不是绿色的，理由是，蓝色的分布虽然其奖励均值比绿色的低（图中相应曲线最大P值对应的Q值），但其实际奖励分布范围较广，由于探索次数的限
制，蓝色单臂对应的行为价值有不少的几率要比分布较窄的绿色单臂要高，也就是说蓝色单臂的行为价值具有较高的不确定性。
因此我们需要优先尝试更多的蓝色单臂，以更准确地估计其行为价值，即尽可能缩小其奖励分布的方差。


单纯用行为的奖励均值作为行为价值的估计进而知道后续行为的选择因为采样数量的原因可能会不够准确，
更加准确的办法是估计行为价值在一定可信度上的价值上限，比如可以设置一个行为价值95%的可信区间上限，
将其作为指导后续行为的参考。如此一个行为的价值将有较高的可信度不高于某一个值：Q+U
这个Q就是经验的Q值，也就是经验奖励r的期望，U就是置信上限，这就引入了UCB算法。

### UCB

UCB就是置信上限区间，从上面的例子中我们知道了，只用均值无法很好的选择动作，因为采样次数的限制，因此要加上一个置信区间，
来使得选择次数少的动作置信区间更大，更容易被选择，因为采样不充分，这个分布和真实分布差距很大，
很有可能充分采样后，均值要比别的动作要高；而选择次数多的，置信区间很小，甚至趋向于0，这代表着这个动作的采样已经很多了，
得到的R的分布已经十分接近真实的分布了，因此只需要看均值就可以了；

那么我们大概知道了，这个U应该是和选择次数成反比的，但是具体怎么表达呢？表达的方式并不唯一（废话），主要看哪个效果更好，
比较常用的效果不错的是根据霍夫丁不等式来得到的，具体如下：

![](https://github.com/cryer/D.Silver_RL_Course/raw/master/images/204.png)

然后把我们的变量带入到不等式中，得到：

![](https://github.com/cryer/D.Silver_RL_Course/raw/master/images/205.png)

令右式为p，并且p随着实践步的进行逐渐变小，不妨设p为t的-4次方，就得到：

![](https://github.com/cryer/D.Silver_RL_Course/raw/master/images/206.png)

这就是我们著名的的UCB1算法：

![](https://github.com/cryer/D.Silver_RL_Course/raw/master/images/207.png)

有个定理：UCB算法可以获得对数的总后悔值。

这就说明UCB算法的效果是很不错的，是可以不断优化算法的。

### UCB算法的效果

下图给出一些算法的比较效果图，可以看出e-greddy参数合适的话可以取得非常好的效果，但是反之，参数不当就爆炸了，但是UCB却总是
可以取得很好的效果。

![](https://github.com/cryer/D.Silver_RL_Course/raw/master/images/208.png)


## 其他的方法

其他的比如利用贝叶斯的方法做概率匹配的，包括信息价值的方法都是不错的方法，但是重要性而言，UCB要高，而且课程中这几个方法
确实都是很快带过的，想了解的可以去看ppt和视频。






